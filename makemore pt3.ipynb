{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build in dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words, block = 3):\n",
    "    block_size = block\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g  = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embd),            generator = g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator = g) * (5/3) / ((n_embd * block_size) ** 0.5) #Kaiming init * 0.1\n",
    "b1 = torch.randn(n_hidden,                        generator = g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator = g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator = g) * 0\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 100000: 1.8910\n",
      "  10000/ 100000: 2.0565\n",
      "  20000/ 100000: 2.2727\n",
      "  30000/ 100000: 2.1771\n",
      "  40000/ 100000: 2.1600\n",
      "  50000/ 100000: 2.7604\n",
      "  60000/ 100000: 2.3876\n",
      "  70000/ 100000: 2.2545\n",
      "  80000/ 100000: 2.2027\n",
      "  90000/ 100000: 1.8810\n",
      " 100000/ 100000: 2.2209\n"
     ]
    }
   ],
   "source": [
    "# Optimisation\n",
    "max_steps = 100000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps+1):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,  Yb)\n",
    "    \n",
    "    # back pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < int(max_steps/2) else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    if i % 10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21a78f10b50>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0929157733917236\n",
      "val 2.1449546813964844\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    h = torch.tanh(embcat @ W1 +b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "montaymyah.\n",
      "seel.\n",
      "mahayla.\n",
      "ren.\n",
      "ruchadraegan.\n",
      "eled.\n",
      "elin.\n",
      "shi.\n",
      "jen.\n",
      "eden.\n",
      "estanara.\n",
      "mykima.\n",
      "kalin.\n",
      "shub.\n",
      "ridhimies.\n",
      "kinde.\n",
      "jennoxrah.\n",
      "casunne.\n",
      "cda.\n",
      "kylene.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647+10)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator = g).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(h.view(-1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21a0cc17990>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full code v1: compilation\n",
    "I have now compiled the original full code, we will make edits on this version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n",
      "11897\n",
      "      0/ 100000: 27.8817\n",
      "  10000/ 100000: 2.8483\n",
      "  20000/ 100000: 2.5429\n",
      "  30000/ 100000: 2.7459\n",
      "  40000/ 100000: 2.1223\n",
      "  50000/ 100000: 2.5109\n",
      "  60000/ 100000: 2.4909\n",
      "  70000/ 100000: 1.9937\n",
      "  80000/ 100000: 2.2218\n",
      "  90000/ 100000: 2.0624\n",
      " 100000/ 100000: 1.9058\n",
      "train 2.176344633102417\n",
      "val 2.202533483505249\n",
      "mora.\n",
      "mayah.\n",
      "seel.\n",
      "nihahlie.\n",
      "emmadceja.\n"
     ]
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)\n",
    "\n",
    "# build in dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words, block = 3):\n",
    "    block_size = block\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n",
    "# MLP\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g  = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embd),            generator = g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator = g) \n",
    "b1 = torch.randn(n_hidden,                        generator = g) \n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator = g) \n",
    "b2 = torch.randn(vocab_size,                      generator = g) \n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    \n",
    "    # Optimisation\n",
    "max_steps = 100000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps+1):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,  Yb)\n",
    "    \n",
    "    # back pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < int(max_steps/2) else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    if i % 10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())  \n",
    "    \n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    h = torch.tanh(embcat @ W1 +b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "split_loss('train')\n",
    "split_loss('val')   \n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647+10)\n",
    "\n",
    "for _ in range(5):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator = g).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full code v2: reduce the biases \n",
    "\n",
    "For the v1, we recognise the biases are random numbers, plugging that into a tanh activation function could lead to multiple 1 or -1 values as tanh(10) = 1.  Values of 1 and -1 would pass the node and will not auto correct. It is best to have the weights close to 0 but NOT 0 to allow for entropy.\n",
    "\n",
    "Bias terms in general can be very close to 0. For the case of logits bias (last bias term, we can initialise it at 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(h.view(-1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n",
      "11897\n",
      "      0/ 100000: 3.3069\n",
      "  10000/ 100000: 2.1900\n",
      "  20000/ 100000: 2.2707\n",
      "  30000/ 100000: 2.4296\n",
      "  40000/ 100000: 1.9041\n",
      "  50000/ 100000: 2.3865\n",
      "  60000/ 100000: 2.3421\n",
      "  70000/ 100000: 1.9668\n",
      "  80000/ 100000: 2.1754\n",
      "  90000/ 100000: 2.0368\n",
      " 100000/ 100000: 1.8858\n",
      "train 2.0854735374450684\n",
      "val 2.1262447834014893\n",
      "montaymya.\n",
      "ziee.\n",
      "medhayla.\n",
      "remmastendraegan.\n",
      "ched.\n"
     ]
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)\n",
    "\n",
    "# build in dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words, block = 3):\n",
    "    block_size = block\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n",
    "# MLP\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "\n",
    "g  = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embd),            generator = g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator = g) * 0.1\n",
    "b1 = torch.randn(n_hidden,                        generator = g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator = g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator = g) * 0\n",
    "\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    \n",
    "# Optimisation\n",
    "max_steps = 100000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps+1):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,  Yb)\n",
    "    \n",
    "    # back pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < int(max_steps/2) else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    if i % 10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())  \n",
    "    \n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    h = torch.tanh(embcat @ W1 +b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "split_loss('train')\n",
    "split_loss('val')   \n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647+10)\n",
    "\n",
    "for _ in range(5):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator = g).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full code v3: Kaiming init\n",
    "\n",
    "Having some intuition of setting the initial biases is great, but we dont want magic numbers. A good way to prevent the magic numbers for the initialisation of the tanh function is to set the formalisation such that the standard deviation of the h sample is 1. This is called the Kaiming Initialisation.\n",
    "\n",
    "Setting the initialisation scale factor of normalisation for the h value biases of 0.1 gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(h.view(-1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot better, much less 1's and -1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n",
      "11897\n",
      "      0/ 100000: 3.3179\n",
      "  10000/ 100000: 2.1910\n",
      "  20000/ 100000: 2.3270\n",
      "  30000/ 100000: 2.5396\n",
      "  40000/ 100000: 1.9468\n",
      "  50000/ 100000: 2.3331\n",
      "  60000/ 100000: 2.3914\n",
      "  70000/ 100000: 2.0292\n",
      "  80000/ 100000: 2.1964\n",
      "  90000/ 100000: 2.0683\n",
      " 100000/ 100000: 1.8759\n",
      "train 2.0623960494995117\n",
      "val 2.114367961883545\n",
      "mon.\n",
      "ammyah.\n",
      "see.\n",
      "med.\n",
      "rylle.\n"
     ]
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)\n",
    "\n",
    "# build in dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words, block = 3):\n",
    "    block_size = block\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n",
    "# MLP\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "\n",
    "g  = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embd),            generator = g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator = g) * (5/3) / ((n_embd * block_size) ** 0.5) # Kaiming init \n",
    "b1 = torch.randn(n_hidden,                        generator = g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator = g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator = g) * 0\n",
    "\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    \n",
    "    # Optimisation\n",
    "max_steps = 100000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps+1):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,  Yb)\n",
    "    \n",
    "    # back pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < int(max_steps/2) else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    if i % 10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())  \n",
    "    \n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    h = torch.tanh(embcat @ W1 +b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "split_loss('train')\n",
    "split_loss('val')   \n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647+10)\n",
    "\n",
    "for _ in range(5):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator = g).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation loss has once again decreased. Get to the same place, but with less magic numbers, and scalable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full code v4: Batch normalisation\n",
    "\n",
    "We want unit batch to be initialised as Gaussians so mean of 0, standard deviation of 1. But we want to allow the batches to move around, so we add a bias so bngain and bnbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 200])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.mean(0, keepdim = True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.std(0, keepdim = True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m n1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(words))\n\u001b[0;32m     33\u001b[0m n2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.9\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(words))\n\u001b[1;32m---> 35\u001b[0m Xtr, Ytr \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m Xdev, Ydev \u001b[38;5;241m=\u001b[39m build_dataset(words[n1:n2])\n\u001b[0;32m     37\u001b[0m Xte, Yte \u001b[38;5;241m=\u001b[39m build_dataset(words[n2:])\n",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m, in \u001b[0;36mbuild_dataset\u001b[1;34m(words, block)\u001b[0m\n\u001b[0;32m     23\u001b[0m         Y\u001b[38;5;241m.\u001b[39mappend(ix)\n\u001b[0;32m     24\u001b[0m         context \u001b[38;5;241m=\u001b[39m context[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m [ix]\n\u001b[1;32m---> 25\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor(X)\n\u001b[0;32m     26\u001b[0m Y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(Y)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)\n",
    "\n",
    "# build in dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words, block = 3):\n",
    "    block_size = block\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n",
    "# MLP\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g  = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embd),            generator = g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator = g) * (5/3) / ((n_embd * block_size) ** 0.5) # Kaiming init \n",
    "b1 = torch.randn(n_hidden,                        generator = g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator = g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator = g) * 0\n",
    "\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    \n",
    "    # Optimisation\n",
    "max_steps = 100000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps+1):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - hpreact.mean(0, keepdim = True)) / hpreact.std(0, keepdim = True) + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,  Yb)\n",
    "    \n",
    "    # back pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < int(max_steps/2) else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    if i % 10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())  \n",
    "\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "   \n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 +b1\n",
    "    hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "split_loss('train')\n",
    "split_loss('val')   \n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647+10)\n",
    "\n",
    "for _ in range(5):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator = g).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalisation has increased significantly, this is very useful for larger scale deep learning frameworks. We can also see we have broken our sampling names as they are illegible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n",
      "12297\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 96\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[0;32m     95\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# update\u001b[39;00m\n\u001b[0;32m     99\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mint\u001b[39m(max_steps\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.01\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\Python311\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Code is broken\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)\n",
    "\n",
    "# build in dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words, block = 3):\n",
    "    block_size = block\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n",
    "# MLP\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g  = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embd),            generator = g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator = g) * (5/3) / ((n_embd * block_size) ** 0.5) # Kaiming init \n",
    "b1 = torch.randn(n_hidden,                        generator = g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator = g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator = g) * 0\n",
    "\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    \n",
    "# Optimisation\n",
    "max_steps = 100000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps+1):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 +b1#no need for bias, already considered in bnbias + b1 \n",
    "    bnmeani = hpreact.mean(0, keepdim = True)\n",
    "    bnstdi = hpreact.std(0, keepdim = True)\n",
    "    hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bnmean_running.mul_(0.999).add_(0.001 * bnmeani)\n",
    "        bnstd_running.mul_(0.999).add_(0.001 * bnstdi)\n",
    "    \n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,  Yb)\n",
    "    \n",
    "    # back pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < int(max_steps/2) else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    if i % 10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())  \n",
    "   \n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 +b1\n",
    "    hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "split_loss('train')\n",
    "split_loss('val')   \n",
    "\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "#--------- EDIT HERE ---------------\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647+10)\n",
    "\n",
    "for _ in range(5):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator = g).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n",
      "12297\n"
     ]
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)\n",
    "\n",
    "# build in dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words, block = 3):\n",
    "    block_size = block\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n",
    "# MLP\n",
    "n_embd = 10 # character embedding vectors\n",
    "n_hidden = 200 # the number of neurons for MLP\n",
    "\n",
    "g  = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embd),            generator = g, requires_grad=True)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator = g, requires_grad=True) * (5/3) / ((n_embd * block_size) ** 0.5) # Kaiming init \n",
    "b1 = torch.randn(n_hidden,                        generator = g, requires_grad=True) * 0.01 # no need for this, since batch normalisation\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator = g, requires_grad=True) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator = g, requires_grad=True) * 0\n",
    "\n",
    "# Batch normalisation parameters\n",
    "bngain = torch.ones((1, n_hidden), requires_grad=True)\n",
    "bnbias = torch.zeros((1, n_hidden), requires_grad=True)\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1,  W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Xb.shape: torch.Size([32, 3])\n",
      "Yb.shape: torch.Size([32])\n",
      "emb.shape: torch.Size([32, 3, 10])\n",
      "embcat.shape: torch.Size([32, 30])\n",
      "hpreact.shape before batchnorm: torch.Size([32, 200])\n",
      "bnmeani.shape: torch.Size([1, 200])\n",
      "bnstdi.shape: torch.Size([1, 200])\n",
      "hpreact.shape after batchnorm: torch.Size([32, 200])\n",
      "h.shape: torch.Size([32, 200])\n",
      "logits.shape: torch.Size([32, 27])\n",
      "loss: 3.3148627281188965\n",
      "\n",
      "torch.Size([27, 10]) True\n",
      "torch.Size([30, 200]) True\n",
      "torch.Size([200]) True\n",
      "torch.Size([200, 27]) True\n",
      "torch.Size([27]) True\n",
      "torch.Size([1, 200]) True\n",
      "torch.Size([1, 200]) True\n",
      "False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m---> 49\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# update\u001b[39;00m\n\u001b[0;32m     52\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mint\u001b[39m(max_steps\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.01\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\Python311\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Optimisation\n",
    "max_steps = 100000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps+1):\n",
    "    \n",
    "    # minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    # linear layer\n",
    "    hpreact = embcat @ W1 #+b1#no need for bias, already considered in bnbias + b1 \n",
    "    \n",
    "    # batch normalisation layer\n",
    "    bnmeani = hpreact.mean(0, keepdim = True)\n",
    "    bnstdi = (hpreact.var(0, unbiased=False, keepdim=True) + 1e-5).sqrt()\n",
    "    hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "    with torch.no_grad():\n",
    "        bnmean_running.mul_(0.999).add_(0.001 * bnmeani.detach())\n",
    "        bnstd_running.mul_(0.999).add_(0.001 * bnstdi.detach())\n",
    "        \n",
    "    # none-linearity\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,  Yb)\n",
    "    print(\"Step:\", i)\n",
    "    print(\"Xb.shape:\", Xb.shape)\n",
    "    print(\"Yb.shape:\", Yb.shape)\n",
    "    print(\"emb.shape:\", emb.shape)\n",
    "    print(\"embcat.shape:\", embcat.shape)\n",
    "    print(\"hpreact.shape before batchnorm:\", hpreact.shape)\n",
    "    print(\"bnmeani.shape:\", bnmeani.shape)\n",
    "    print(\"bnstdi.shape:\", bnstdi.shape)\n",
    "    print(\"hpreact.shape after batchnorm:\", hpreact.shape)\n",
    "    print(\"h.shape:\", h.shape)\n",
    "    print(\"logits.shape:\", logits.shape)\n",
    "    print(\"loss:\", loss.item())\n",
    "    print()\n",
    "\n",
    "    # back pass\n",
    "    for p in parameters:\n",
    "        print(p.shape, p.requires_grad)\n",
    "        p.grad = None\n",
    "    print(loss.requires_grad)\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < int(max_steps/2) else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    if i % 10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n",
      "Total parameters: 12297\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[0;32m     93\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Update\u001b[39;00m\n\u001b[0;32m     97\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m (max_steps \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.01\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\Python311\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# Load dataset\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "# Build vocabulary\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)\n",
    "\n",
    "# Dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words, block=3):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n",
    "# MLP parameters\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g, requires_grad=True)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g, requires_grad=True)\n",
    "W1.data *= (5/3) / ((n_embd * block_size) ** 0.5)\n",
    "b1 = (torch.randn(n_hidden, generator=g) * 0.01).requires_grad_()\n",
    "W2 = (torch.randn((n_hidden, vocab_size), generator=g) * 0.01).requires_grad_()\n",
    "b2 = (torch.randn(vocab_size, generator=g) * 0).requires_grad_()\n",
    "\n",
    "# Batchnorm parameters\n",
    "bngain = torch.ones((1, n_hidden), requires_grad=True)\n",
    "bnbias = torch.zeros((1, n_hidden), requires_grad=True)\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(\"Total parameters:\", sum(p.nelement() for p in parameters))\n",
    "\n",
    "# Optimisation\n",
    "max_steps = 100000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps+1):\n",
    "    \n",
    "    # Minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    emb = C[Xb]                           # (batch_size, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1)    # (batch_size, block_size * n_embd)\n",
    "    hpreact = embcat @ W1                  # (batch_size, n_hidden)\n",
    "\n",
    "    # Batchnorm\n",
    "    bnmeani = hpreact.mean(0, keepdim=True)\n",
    "    bnstdi = (hpreact.var(0, unbiased=False, keepdim=True) + 1e-5).sqrt()\n",
    "    hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bnmean_running.mul_(0.999).add_(0.001 * bnmeani.detach())\n",
    "        bnstd_running.mul_(0.999).add_(0.001 * bnstdi.detach())\n",
    "\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    lr = 0.1 if i < (max_steps // 2) else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # Logging\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "# Loss after training\n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1\n",
    "    hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "\n",
    "# Sampling new names\n",
    "g = torch.Generator().manual_seed(2147483647+10)\n",
    "\n",
    "for _ in range(5):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        embcat = emb.view(1, -1)\n",
    "        hpreact = embcat @ W1\n",
    "        hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "        h = torch.tanh(hpreact)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
